\section{Related Work}
\label{sec:related}

%?: ~\cite{7158965}.
%?: ~\cite{dockerbook}.
%?: ~\cite{5655241} - dedup of containers?
%Due to its increasing popularity, Docker has recently received increased
%attention from the research community.

A number of studies investigated various dimensions of Docker storage
performance\cite{slacker,docker-driver-eval,improve-cow-container-drivers,dockerworkload,dedupanalysis,shifter,exoclones}. However, none
of them provide deduplication capabilities for the registry.

Skourtis \etal~\cite{skourtis2019carving} proposed to reduce registry storage utilization by
restructuring layers to maximize their overlap. However, they change
the existing structure of images whereas \sysname leaves images unchanged.
%
%Many studies focus on reducing the size of Docker
%images~\cite{rastogi2017cimplifier,gschwind2017optimizing} to reduce image pulling time.
%
%DockerSlim~\cite{dockerslim} is an effective tool to minify a single image.
%
There is one community proposal to add file-level deduplication to container
images~\cite{Krohmer-proposal}, but doesn't provide a detailed design or performance
analysis.
%https://gist.github.com/devkid/5249ea4c88aab4c7bff1b34c955c1980#file-readme-md
%However, neither of these approaches provide a performance evaluation.
%\Ali{Slacker does provide performanve evaluation. Perhaps, we can say that none of these papers
%provide registry level deduplication and performance evaluation to quantify deduplication
%overhead.}
%
There is other work aimed at reducing image sizes~\cite{cntr,rastogi2017cimplifier,gschwind2017optimizing,dockerslim} which are orthogonal to our approach and can be used  with \sysname.
%
%Harter \etal~\cite{slacker} studied 57 images from Docker Hub for a variety of metrics
%but not for data redundancy.
%The authors used the results from their study to derive a benchmark to evaluate
%the push, pull, and run performance of Docker graph drivers based on the studied
%images. Compared to Slacker, our analysis focuses on the entire Docker Hub dataset.
%
%Cito \etal~\cite{cito2017empirical} studied 70,000
%Dockerfiles and focused on the image build process not its contents.
%of Docker ecosystem with a focus on prevalent quality issues, and the evolution of Docker 
%files based on a data set of 70,000 Docker files.
%However, their study did not focus on actual image data.
%
%Shu \etal~\cite{dockervulnerabile} studied the security vulnerabilities in Docker Hub
%for 356,218 images.
%
%Anwar \etal~\cite{dockerworkload} performed a detailed analysis
%of an IBM Docker registry workload but not the dataset.
%
% and found there is a strong need for more
%automated and systematic methods of applying security updates to Docker images. While
%the amount of images is similar compared to our study, Shu \etal focused on a subset
%of 100,000 repositories and different image tags in these repositories.
%
%Dockerfinder~\cite{dockerfinder} is a microservice-based prototype that allows searching
%for images based on  multiple attributes, e.g., image name, image size, or supported software 
%distributions. It also crawls images from remote Docker registry but the authors do
%not provide a detailed description of their crawling mechanism.
%
%Bhimani~\cite{dockerssd} \etal characterized the performance of persistent storage options
%for I/O intensive containerized applications with NVMe SSDs.
%
%Data deduplication is a well
%explored and widely applied technique~\cite{2009-sparse_indexing_inline_dedup_using_sampling-fast,
%2001-low_bandwidth_network_fs-sosp,
%2012-idedup-fast,
%tarasov2014dmdedup,
%2008-avoid_disk_bottleneck_data_domain_dedupfs-fast}.
%
%A number of studies which focus on
%real-world datasets~\cite{2009-dedup_effectiveness_on_vm_disk_images-systor,
%2012-data_reduction_in_primary_storage-systor,
%2012-hpc_practical_dedup_study-sc,
%2013-charact_increment_changes_data_protect-atc,
%msst16dedup-study,
%2012-charact_backup_workloads-fast,
%2013-charact_dedup_effic_big_data-iiswc}
% can be complementary to our approach.
%but to the best of our knowledge, we are the first to analyze a large-scale Docker
%registry dataset for its deduplication potential.
%and propose to apply deduplication to it.

Deduplication in cloud storage has been investigated for decades, particularly for virtual machine
images~\cite{zhou2013characterizing,srinivasan2012idedup,jin2009effectiveness, jayaram2011empirical}.
%
%For instance, Jayaram~\etal~\cite{jayaram2011empirical} studied 525 images
%from a production IaaS cloud and detected similarities. 
 %...because current hypervisor-based virtualization techniques do not support
 %block sharing between virtual disk images, instead relying on techniques such
 %as overlays to build multiple VMs from a single "base" image, similar with
 %Docker images.
Many studies also focused on primary and backup data
deduplication~\cite{tarasov2014dmdedup,muthitacharoen2001low,lu2012insights,2009-sparse_indexing_inline_dedup_using_sampling-fast,2013-charact_increment_changes_data_protect-atc,wallace2012characteristics,zhu2008avoiding, lillibridge2013improving,
fu2014accelerating,fu2015design,fu2011aa}
and show the effectiveness of file- and sub-file-level
deduplication~\cite{2012-hpc_practical_dedup_study-sc,msst16dedup-study}.
%
\sysname utilizes file-level deduplication but is specifically designed for Docker registries,
which allows it to leverage image and workload information to reduce deduplication overhead.
%
%\Ali{Id whole-file deduplication a term? If not we should stick with file-level deduplication.}


%The performance of restoring the original data post-deduplication is as important as the deduplication ratio  even for backup systems~\cite{lillibridge2013improving}.
%%\Ali{What is restoring performance?}
%%\Subil{reworded. is this better?}
%%
%To improve the restoring latency, multiple optimizations are such as caching, prefetching, exploiting
%historical information and temporal trends, data locality etc. are evaluated~\cite{fu2014accelerating,fu2015design,fu2011aa}. 
%%
%\sysname combines several of these optimizations in its design to provide efficient
%image deduplication.
%
%All these optimization techniques inspire our design.
%\Ali{Related work needs to be expanded.}
%Most existing cloud storage providers employ data
%deduplication techniques to eliminate redundant data, same data stored more
%than once.

%Deduplication techniques significantly reduce storage needs and therefore
%reduce storage costs and improve storage efficiency.

%Data deduplication works by storing duplicate data chunks only once, keeping
%only the unique data chunks. 
%
%Current cloud providers deploy a cross-user client-side fixed-size-chunk-level
%data deduplication that delivers the highest deduplication
%gain~\cite{pooranian2018rare}.
%
%These approaches maximize the benefit of deduplication: The cross-user data
%deduplication treats cloud storage as a pool shared by all the cloud users,
%because the potential for data deduplication is the highest as the probability
%for redundancies and duplicates is higher the more inclusive the shared pool.
%
%The fixed-size-chunk-level specifies that a fixed-size chunk is the unit for
%checking for duplicates on cloud storage.
%
%Google cloud and AWS employ StorReduce, a deduplication software that performs
%in-line data deduplication transparently and resides between the client's
%application and the hosting cloud storage.
%
%StorReduce provide 80-97\% storage and bandwidth reduction to the cloud
%providers~\cite{StorReduce_google}.


%Unlike our study, their analysis
%is focused on the execution of containers rather than on their storage at the registry side.

%\textbf{Analysis on file systems}:
%File system contents have been widely studied in different operating system environments.
%Douceur and Bolosky collected and analyzed over ten-thousand Windows file
%systems~\cite{largefscontent}. They found that the size of file and directory are fairly
%consistent across file systems, but file lifetimes and file-name extensions vary based on
%the job function of the users. 
%Agrawal, Bolosky, Douceur, and Lorch collected and analyzed a five-year file-system metadata
%over 60,000 Windows file systems, and presented the temporal trends relating to file size,
%file type, and directory size~\cite{fiveyearfsmetadata}.
%Sienknecht, Friedrich, Martinka, Friedenbach collected file system data from UNIX systems
%based on a dataset of 46 systems, 267 file systems, 151,000 directories and 2,300,000 files
%and found small files dominate in count~\cite{distributedatainfs}.
%%Traeger, Zadok, Joukov, and Wright surveyed 415 file system and storage benchmarks from
%106 recent papers~\cite{xxx}.  
