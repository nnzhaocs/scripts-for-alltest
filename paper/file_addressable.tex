\section{Rethinking storage model for Docker registry}
\label{sec:file_adressable}

%\subsection{}
%\paragraph{Benefit of file-level content addressable storage}
Currently, Docker registry uses layer-level content addressable storage model to avoid storing redundant copies of an identical layer. 
However, as discussed in previous sections, almost 97\% files are redundant files which accounts for 86\% of storage space (uncompressed). 
As the number of images stored in Docker registry is increasing dramatically, 
storing and managing the large proportion of redundant files requires both storage equipment and administration investment. 

In this section, a file-level content addressable storage model is suggested for Docker registry to remove redundant files and save space.   
%Docker registry can save a lot of space if using file-level content addressable storage. 
%Benefit: saving money, reducing storage management overhead.

\subsection{File-level content addressable storage model (FLCAS)}
\label{subsec:FLCAS}

\begin{figure}
	\centering
	\includegraphics[width=0.3\textwidth]{graphs/graph_compression_layers.pdf}
	\caption{File-level content addressable model.
	}
	\label{fig:file-dedup-model}
\end{figure}

File-level content addressable storage system (FLCAS) uses file-level dedup to remove the redundant copies of an identical file by generating a digest (such as MD5 or SHA256) from the file it refers to. FLCAS exposes a high-level layer addressing by mapping their digests to their containing file digests.     
 %a digest generated by a cryptographic hash function (such as ) 

%\subsection{Trade-off discussion}
File-level dedup can significantly reduce the redundant files in docker registry and save a large volume of storage space. However, it also introduces considerable overhead, which including layer decompression, file content digest calculation, searching, and layer compression overhead. These additional operations are either CPU intensive or I/O intensive.  

\begin{figure}
	\centering
	\includegraphics[width=0.4\textwidth]{graphs/pull-cnt.pdf}
	\caption{CDF of layer \& image pull count.
	}
	\label{fig:pull-cnt}
\end{figure}

\paragraph{Caching hot layers}To reduce overhead, we temporarily cache the hot/recently requested layers as gzip compressed tar files in a \textit{layer cache pool} without file-level dedup.
We observed that only a small proportion of images and layers are frequently requested. A majority of images and layers are \textit{cold}.
As shown in Figure~\ref{fig:pull-cnt}, x-axis shows the total number of pullings since the layers/images are stored in Docker Hub to May 30, 2017. 
We see that only 20\% and 10\% of images are pulled more that 100 and 360 times respectively. 
Similarly, only 20\% and 10\% of layers are pulled more that 217 and 660 times. 
Note that 
the layers'pull count shown in Figure~\ref{fig:pull-cnt} is calculated by aggregating all the images'pull counts that refers this layer.
Note that the image pull counts are crawled from Docker Hub website. 
Actual layer pull count should be less than the number shown in Figure~\ref{fig:pull-cnt} because pulling a image does not necessarily pull all its containing layers as we don't pull the layers if they have already been downloaded.
 
\paragraph{Dedup when workload is light}Consider that dedup is expensive in terms of performance overhead, file-level dedup is only triggered for removing the redundant files for cold layers when the workload is lower than a predefined threshold $\sigma_{wl}$ and storage utilization is higher than a predefined threshold $\sigma_{su}$. 
Typically, workload fluctuates, with peaks and troughs. According to IBM bluemix workload analysis\cite{xxx}, xxx of time, there were only xxx requests. %During high intensive load, 
Thus, file-level dedup runs periodically to reduce redundant files without causing performance overhead. 

%high while start file-level dedup 
\paragraph{FLCAS model}
Figure~\ref{fig:file-dedup-model} shows an example of FLCAS. The recently pushed layers are treated as hot layers and cached in layer cache pool. 
All the pulling requests to these hot layers are served from layer cache pool.
When the workload is lower than $\sigma_{wl}$ and storage utilization is higher than $\sigma_{su}$, file-level dedup is triggered. 
File-level dedup mainly contains three steps: layer decompression, file content digest calculation, and searching \&. indexing.
During file-level dedup, the unique files are stored in a \textit{file pool} and redundant copies are removed.  
A unique \textit{file content address} is recorded, which is a
file digest uniquely and linked to the files'location.
Meanwhile, a layer digest to its containing file content digest mapping record is also created and saved to a \textit{layer-to-file digest table}. 
 All the pulling request to the cold layers are served by file pool via searching for the layer digest from layer-to-file digest mapping table. After fetching all the containing files that are required by requested layer, FLCAS compresses these files as gzip file before sending it to Docker clients.         

\subsection{Off-line FLCAS latency} 

Since file-level dedup involves operations that are either CPU intensive or I/O intensive, fast CPUs and high capacity RAMs are required to improve performance.

\paragraph{Using RAM to load \&. process layers}

To improve performance, we use RAM to temporarily store \textit{small} layers and directly process them in RAM.
Specially, we first load small layers in a RAM disk and perform decompression, unpacking, file content digest calculation in RAM, and remove them from RAM after completion.   
According to our findings that majority (87.3\%) of layers that are less than 50M as shown in Figure~\ref{fig:image-layer-size}. So majority of layers can be stored and processed in RAM to speed up file-level dedup. 
%layers that are less than 50M are stored and processed in RAM while the rest layers are stored and processed in SSDs. 

\paragraph{FLCAS prototype}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{graphs/res-time.pdf}
	\caption{Off-line file-level dedup latency.
	}
	\label{fig:dedup-res}
\end{figure}

%\paragraph{Image \&. layer popularity skewness} 
To quantify the latency of file-level dedup, we simulated a one-node docker registry with 64 RAM and 32 Cores with 60 concurrent pulling requests to cold layers since dedup process only starts periodically when the workload is lower and only cold layers are evolved in dedup process (discussed in Section~\ref{subsec:FLCAS}).

We wrote around 600 lines of Python code to simulate file-level dedup operations on the layer dataset, specially, it reads a layer from layer dataset to a RAM disk (i.e., tmpfs) and decompresses the layer in the RAM disk. Then it calculates each containing files' digests and append the layer-to-file digest mapping records to a mapping table. To improve searching performance, the mapping table is stored in Hive database~\cite{xxx}. To speed up dedup process, we leverage multi-processing. Only unique files are maintained in RAM disk while the redundant copies are removed from RAM disk. To simulate concurrent pulling requests to the cold layers, we randomly generate 60 different cold layer digests and each process takes one layer pulling request, then packs and compresses the layer's containing files by searching the mapping table for the layer digest. We measured its performance by using file-level dedup for 0.89 million small layers.

\paragraph{File-level dedup latency}

%and pulling requests for  
%
Figure~\ref{fig:dedup-res} shows the breakdown latency distribution for each involved operation: decompression time, unpacking time, file content digest calculation time, and searching time.

First, among all the operation latencies, we see that searching time is the smallest. 80\% of searching time is less than 0.004 s. The mapping table maintains 0.98 million layer-to-file digest mapping records. Consider that more than 1.7 million layers are stored in Docker hub and the number is still increasing, it's better to choose a fast distributed database to provide high searching performance and scalability.
  
Second, we see that digest calculation time spreads over a large range started from 0.000005 s to 124.7 s. It's because digest calculation time largely depends on the layer size. Typically, smaller layers contains a smaller number of smaller files, which takes much less time to calculate their digests. While if the layer is bigger, the digest calculation overhead will be higher. 80\% of digest calculation time is less than 4.21 s. 
%The layer file digest calculation can be finished within 10 s on average. 
Thus, we suggest that multiple-threading is needed to calculate the files' digests simultaneously; Fast CPUs as well as more powerful computing nodes are required to speed up digest calculation.

Third, the latencies for decompression and unpacking have the same distribution during the lowest response time range started from 0.04 s to 0.15 s. Around 60\% of decompression and unpacking time are less than 0.15 s. However, decompression has the highest latencies than that of unpacking. 80\% of decompression is less than 0.55 s while 80\% of packing time is less than 0.21 s. 

Figure~\ref{fig:dedup-res} also shows the total time distribution for file-level dedup which is the sum of latencies for decompression, unpacking, digest calculation, and searching. We see that 80\% of file-level dedup time is less than 9.09 s. We also measured the throughput of 60 processes. Our one-node file-level dedup prototype can process about 3 layers/s. We suggest to use more high powerful machines to improve throughput.

\paragraph{Pulling latency}
Note that the pushing performance are not influenced by the file-level dedup because once the layer reaches registry, an response message will be sent to the user. No addition delay will be added to the pushing time. However, compression time will impact pulling performance since the files need to be packed and compressed first and then sent to the users once registry receives pulling requests to the cold layers. 

Figure~\ref{fig:dedup-res} shows the pulling time distribution. Note that the pulling time is the sum of archiving time, compression time, and searching time and does not include network transfer time. We can see that 
compression and archiving time almost share the same distribution during the lowest response time range started from 0.04 s to 0.15 s. 60\% of compression and archiving time are less than 0.15 s. While compression has the highest latency. 80\% of compression time is less than 2.82 s. We see that archiving time and compression contributes equally to pulling time when their latencies are lower than 0.15 s while compression time almost equals to pulling time when the latency is greater than 0.15 s. Hence, we suggest that fast compression methods are required to reduce compression time.  
 
%majority (xxx) of layers'dedup times are less than xxx.
%during peak workload, which means that we start file-level dedup whenever it receives a layer. 
%To simulate the high intensive workload, we first sent a sequential of layer pushing requests to registry and stored, then we measure the file-level dedup  
%, archiving time, and compression time.
%Second, compare the latency for each operation, we see that xxxx
%\paragraph{Latency breakdown}
%We calculated the latency for each operation for all layers as shown as Table~\ref{tbl:latency_breakdown}.
%Figure~\ref{xxx} shows the compression time across 

%\begin{figure}[!t]
%	\centering
%	\subfigure[CDF of repositories by pull count]{\label{fig_pull_cnt_total}
%		\includegraphics[width=0.23\textwidth]{graphs/pull_cnt.pdf}%
%	}
%	\subfigure[Histogram of repositories by pull count]{\label{fig_pull_cnt_count}
%		\includegraphics[width=0.22\textwidth]{graphs/count_pull_cnt.pdf}
%	}
%	\caption{Repository popularity distribution}
%	\label{fig-pop}
%\end{figure}

%=======================================
%|             OLD VERSION              |
%=======================================

%\paragraph{Latency distribution for each operation}
%\subsubsection{When to start file-level dedup?} 

%\paragraph{Latency distribution for each operation}

%\paragraph{Small compression ratio and small layer size}
%
%\input{fig-char-layer-compression}
%
%\input{fig-char-layer-sizes}
%
%We found that most layers'compression ratio is really lower (?) while most of layers have a smaller size. 
%So how about we use archiving instead of compression if the network speed is higher (?GB/s)?

%\paragraph{Network transfer speed is high!}

%\subsubsection{File-level content addressable storage for cold layers}

%\begin{figure}
%	\centering
%	\includegraphics [width=0.45\textwidth]{plots/exp-total-stev-erase.eps}
%	\subfigure[]{\label{fig:per_layer_ratio_fcnt_cdf}
%		\includegraphics [width=0.23\textwidth]{graphs/}
%	}
%	\subfigure[Similar layer dedup]{\label{fig:per_layer_ratio_fcnt_pdf}
%		\includegraphics [width=0.22\textwidth]{graphs/graph_reconstruct_layers.pdf}
%	}
%	\caption{File-level content addressable storage model}
%	\label{fig:eval-stdev-erasure-cnt}
%\end{figure}

%\subsection{Hints for performance improvement and storage saving}

%\begin{table} 
%	\centering 
%	\scriptsize  
%	%\begin{minipage}{.5\linewidth}
%	\caption{Latency breakdown} \label{tbl:latency_breakdown} 
%	\begin{tabular}{|l|l|l|l|l|}%p{0.14\textwidth} 
%		\hline 
%		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ... 
%		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ... 
%		Operations/latency (S) & max & min & median & avg.\\
%		\hline
%		 gunzip decompression (RAM) & 257.16  & 0.04  & 0.15  & 0.39 \\
% 		\hline
% 		tar extraction (RAM) & 43.41  & 0.04  &  0.14  & 0.18 \\
%		\hline
%		Digest calculation (RAM) & 3455.01  & $<$0.00  & 0.05 & 10.65 \\
%		\hline
%		tar archiving (RAM)  & 53.44 & 0.04 & 0.14 & 0.19\\
%		\hline
%		gzip compression (RAM) & 496.04 & 0.04 & 0.15 & 2.10 \\
%%		\hline
%%		Total time (RAM) (with compression) & & & & \\
%%		\hline
%%		Total time (RAM) (without compression) & & & & \\
%		\hline
% 		\hline
% 		gunzip decompression (SSD) &   &   &    &  \\
% 		\hline
% 		tar extraction (SSD) &   &   &    &  \\
%		\hline
%		Digest calculation (SSD) &  &  & & \\
%		\hline
%		tar archiving (SSD) &  &  & & \\
%		\hline
%		gzip compression (SSD) & &  &  & \\
%%		\hline		 
%%		Total time (SSD) (with compression) & & & & \\
%%		\hline
%%		Total time (SSD) (without compression) & & & & \\
%		\hline
%		\hline
%		Network transfer & 20587.94 & $<$ 0.00 & $<$ 0.00 & 1.20 \\
%		\hline 	
%	\end{tabular} 
%\end{table}


%\begin{table} 
%	\centering 
%	\scriptsize  
%	%\begin{minipage}{.5\linewidth}
%	\caption{Summary of layer \& image characterization} \label{tbl:redundant_ratio} 
%	\begin{tabular}{|l|l|l|l|l|}%p{0.14\textwidth} 
%		\hline 
%		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ... 
%		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ... 
%		Metrics & max & min & median & avg.\\
%		\hline
%		Compressed layer size &   &   &   &  \\
%		\hline
%		Uncompressed layer size &   &   &    &  \\
%		\hline
%		Archival size &  &  & & \\
%		\hline
%		Compression ratio &   &   &    &  \\
%		\hline
%		Layer pull cnt. &  &  & & \\
%		\hline
%		File cnt. per layer &  &  & & \\
%		\hline
%		Dir. cnt. per layer &  &  & & \\
%		\hline
%		Layer depth &  &  & & \\
%		\hline
%		\hline
%		Compressed image size &  &  & & \\
%		\hline
%		Uncompressed image size & &  &  & \\
%		\hline
%		Archival image size & &  &  & \\
%		\hline
%		Compression ratio &   &   &    &  \\
%		\hline
%		Image pull cnt.  &  &  & & \\
%		\hline
%		Layer cnt. per image  &  &  & & \\
%		\hline
%		Shared layer cnt. per image  &  &  & & \\
%		\hline
%		File cnt. per layer &  &  & & \\
%		\hline
%		Dir. cnt. per layer &  &  & & \\
%		\hline	
%	\end{tabular} 
%\end{table} 

%\subsection{Constructing shared layers for redundant directories/files}
%
%\paragraph{Smaller number of layers are shared among different images}
%\input{fig-repeat-layer-cnt.tex}
%
%\paragraph{Smaller pull latency than recompression model} the registry can prepare the reconstructed layers before users issue a pull request. But this model requires users to rebuild two layers.

%\subsubsection{Summary of Suggestions/trade-offs between dedup ratio and recompression overhead}
%
%\paragraph{1. using archiving instead of compression}
%\paragraph{2. using file-level dedup for cold images/layers}
%\paragraph{3. using file-level dedup economically}
%When to trigger file-level dedup?
%\paragraph{4. constructing shared layers for redundant dirs/files, for example,}
%%\subsection{Layer reconstruction model}
%%\subsubsection{Reconstruction overhead}
%%\subsubsection{Trade-offs between dedup ratio and reconstruction overhead}
%%\paragraph{Dedup ratio VS. Rebuild overhead}
%%\subsection{Evaluation results}