
%In the following, we first present the performance comparison between original registry and \sysname.
%Next, we present the layer restoring performance.

\subsection{Restore performance of the deduplication cluster}
\label{sec:eval-dedup}

%We map the traces with two different layer groups: 
%small layers (layer size $\leq$ 50 MB)

\input{fig-eval-1node}

In this experiment,
we measure the layer restoring latency of \sysname's deduplication cluster
and its impact on \texttt{GET} layer latency.
We first measure a single P-server's layer restoring capability
and compare it with a \emph{No deduplication} scheme --
downloading layers from server's local file system without deduplication.
%denoted as .
Then we scale out to multiple P-servers
and compare it with a \emph{CHT based distributed object storage} system.
%
%
% (denoted as
%\sysname-dedup)
% and 
%overhead of layer deduplication on .
%To measure the  
%We compare the \texttt{GET} layer latency of D-servers
%with 
%by running \sysname with \text{only deduplication} on a single D-server.
%We launch a client on another server and \texttt{pulls} 3 layers in parallel.
%We configure \sysname as
%registry without deduplication,
%deduplication registry,
%deduplication registry with LRU cache~\cite{xxx},
%and
%deduplication registry with a preconstruct cache.
%We set the cache size as 20\% of ingress data.

\paragraph{Restoring latency}
We launch a single node registry on a server.
and a client on another server.
We compare 4 backends:
(1) Local file system with \emph{no deduplication},
(2) Local file system with \sysname \emph{Deduplication} but without \emph{cache},
(3) Local file system with \sysname \emph{Deduplication} and a \emph{ARC cache}, and
(4) Local file system with \sysname \emph{Deduplication} and a \sysname \emph{preconstruct cache}
as shown in Figure~\ref{fig:eval-1nodegetlayerlatency}.
%
The client matches 2500 requests from \texttt{Dal} to layers with different sizes
and replays them to the D-server.
%We compare four backends: 

As shown in Figure~\ref{fig:eval-1nodegetlayerlatency},
layer restoring increses the average \texttt{GET} layer latency by 189\% for layers with size of 1 MB
 (\emph{see line Deduplication without cache})
compared with \emph{No deduplication}.
Moreover,
layer restoring latency increases as layer size.
It takes 0.22 s to restoring and download layers with size of 9 MB.

\paragraph{Preconstruct cache VS. ARC cache}
%\paragraph{Cache}
As shown in Figure~\ref{fig:eval-1nodegetlayerlatency}, 
%the average \texttt{GET} layer request latency increase with the layer size.
adding a cache can largely reduce layer restoring latency.
By using a ARC cache, the layer restoring overhead decrease by
40\% for layers with size of 1 MB compared with \emph{Deduplication without cache}.
However,
as layer size increase, 
the improvements of \texttt{GET} layer request latency by using
ARC cache drops.
For layer size equals to 10 MB,
%
ARC cache only reduces layer restoring overhead by 16\%.
%ARC cache can only improve 10\% of 
%Restoring a layer puts 150\% overhead on \texttt{GET} layer latency.
%By adding a ARC cache,
%he average \texttt{GET} layer latency decreases by 39\%.
With \sysname preconstruct cache,
the average \texttt{GET} layer latency descreases by 24\% over an ARC cache
for layers with size of 1 MB.
%Overall,
Compared to \emph{No deduplication}, 
\sysname with preconstruct cache only adds 19\% of overhead for layers with size of 9 MB. 

\paragraph{Cache hit ratio}
Figure~\ref{fig:eval-cachehitratios}
shows the cache hit ratio for ARC cache and Preconstruct cache.
Note that the cache size is set to 20\% of ingress data.
The cache hit ratio for ARC is stable at 0.77 among different layer sizes.
%However, 
Preconstruct cache achieves a hit ratio of 0.95 for 1 MB.
With the layer size increases to 9 MB,
preconstruct cache hit ratio decreases to 79\%. 
This is because layer restoring latency increases with layer size as 
shown in Figure~\ref{fig:eval-1nodegetlayerlatency}
and more layers cannot be preconstructed \emph{on time} as layer size becomes bigger. 
As shown in Figure~\ref{fig:eval-cachehitratios},
the number of waiting \texttt{GET} layer requests increases as layer size (see preconstruct cache waiting ratio).
Since the layer construction already starts before these requests arrives,
the restoring overhead can also be largely reduced.
 The user-bahavior based request prediction accuracy is calculated by sum of preconstruct cache hit ratio and waiting ratio.
 As shown in Figure~\ref{fig:eval-cachehitratios},
 the prediction accuracy is 0.95.
 
\paragraph{Impact of duration between a \texttt{GET} manifest request and its subsequent \texttt{GET} layer requests}
Next, 
we vary 
the duration between a \texttt{GET} manifest request and its subsequent \texttt{GET} layer requests (denoted as durationML).
The layer size used here is 9 MB.
%
Figure~\ref{fig:eval-durationML} shows 
the average \texttt{GET} layer latency for \sysname deduplication with preconstruct cache and 
\emph{No deduplication}.
We see when the durationML is 1 s, 
\sysname with preconstruct cache only increases 19\% of overhead.
However, as the durationML decreases,
the average \texttt{GET} layer latency increases as more layer cannot be preconstructed on time.
When the client replays requests as fast as possible,
the overhead of restoring layers on \texttt{GET} layer requests increases by 30\%.

%the cache hit ratio of the ARC cache and the preconstruct cache.

\paragraph{Cluster scale impact}
We scale the backend storage system 
to show how the \texttt{GET} layer performance changes.
 %To do that,
 %we launch multiple registries on each registry server. 
 The backends are:
 (1) \sysname deduplication cluster (\sysname-dedup),
 and
 (2) Consistent hashing based distributed object store. 
 As mentioned in \S\ref{sec:impl},
 we implement layer deduplication on the
 local file system of each registry server.
We implement consistent hashing logic on client side (see \S\ref{sec:impl}) to
 scale the registry server as a deduplication cluster.
 %Besides, 
 %that the client distributes layers to 
Therefore,
we also use our client to scale the local file system of each registry server
as a \emph{ consistent hashing based distributed object store}.
We increase the number of registry servers from 7 to 14,
and the layer sizes from 30 to 70.
We launch 20 clients to replay 2500 requests to registries.
%
%\sysname deduplication cluster and test the layer restoring performance.
As shown in Figure~\ref{fig:eval-clusterscale},
%when the cluster size is 7,
when the number of servers increases,
The average \texttt{GET} layer latency from
distributed object store doesn't changes much.
However, the performance of \sysname scales with the number of servers.
This is because with a bigger deduplication cluster,
 layer restoring latency is largely reduced as 
 layer restoring has a higher parallelism level.
 As shown in Figure~\ref{fig:eval-clusterscale},
 when layer size is 30 MB,
 double the cluster size can reduce 35\% of restoring overhead.

\paragraph{Restoring latency breakdown}

Figure~\ref{fig:eval-restoringbreakdown} shows 
the layer restoring latency breakdown.
The layer restoring latency contains
layer recipe lookup duration,
layer construction duration, and layer transfer duration.
Layer construction duration
contains slice recipe lookup duration,
slice construction, and slice transfer duration.
%
The latencies are measured on the 7-node cluster.
The layer restoring latencies shown in Figure~\ref{fig:eval-restoringbreakdown}
only includes the complete layer restoring processes
and we eliminate the layer restoring processes that are waiting for
others to construct layers.
Note that the layer restoring latencies measured also includes
layer restoring processes for preconstruction.

As shown in Figure~\ref{fig:eval-restoringbreakdown},
slice construction duration accounts for the largest partition of  
layer construct time.
Slice construction contains
file archiving and compression, which are two bottlenecks of slice construction. 
Layer construct time increases with layer size because 
the layer slices become bigger and take longer time to archive and compress.
For example,
for layer size equals 30 MB,
fetching and merging slices takes 1.3 s on average.
When the layer size increases to 70 MB,
it takes 3.6 s on average to fetch and merge slices.

\paragraph{Client concurrency impact}
We increase the number of concurrent client request
and measure the \texttt{GET} layer request latency on 14-node cluster
for \emph{\sysname-dedup} and \emph{distributed object storage system} respectively.
%
Figure~\ref{fig:eval-clientscale} shows
the average \texttt{GET} layer latency for \emph{\sysname-dedup}
and \emph{distributed object storage system} with different number of concurrent client requests.
As shown, 
distributed storage system shows almost a stable \texttt{GET} layer latency with increasing number of clients.
However,
as the number of concurrent client requests increases,
the average \texttt{GET} layer request latency of \emph{sift-dedup} increases.
For example,
when the number of concurrent client requests is 20,
the average \texttt{GET} layer request latency for \emph{\sysname-dedup} is 0.37 s.
When the number of concurrent client requests increases to 60,
the average \texttt{GET} layer request latency for \emph{\sysname-dedup} increases to 0.47 s.
This is because layer restoring process is computation-intensive.
With more concurrent requests,
the layer restoring overhead grows higher.
\paragraph{Deduplication latency breakdown}
Figure~\ref{fig:eval-dedupbreakdown}
shows the breakdown of deduplication latency.
Note that to reduce the impact of layer deduplication on
\texttt{GET} layer request,
 layer deduplication process is a 
single-threaded off-line deduplication.
As shown in Figure~\ref{fig:eval-dedupbreakdown},
decompression and file-level deduplication
account for the largest proportion of layer deduplication duration.
This is because \sysname uses single-threaded decompression
and file-level deduplication for reducing the computation overhead of the off-line layer deduplication.
Moreover, the duration of decompression and file-level deduplication
increases with layer sizes.
For example,
when the layer size increases from 30 MB to 70 MB,
the decompress duration increases from 1.6 s to 3.8 s on average
and the file-level deduplication duration increases from 2.5 s to 8.8 s.
%\paragraph{Metadata size} 
%Figure~\ref{xxx} shows the average metadata size on each registry server measured by using the size of the used memory in Redis~\cite{redis} when in \emph{B-mode 0}.
%Note that the replication level for the Redis cluster is also set to three.
%As shown, for all traces, the total size of the metadata generated by \sysname such as \emph{recipes} for layers or slices, \emph{indexes} of layers or files, and RLmap or ULmap is less than 100 MB.
%With the 16 GB memory on each registry server, such metadata size is negligible.


