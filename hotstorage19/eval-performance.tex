
%In the following, we first present the performance comparison between original registry and \sysname.
%Next, we present the layer restoring performance.

\subsection{Restore performance of the deduplication cluster}
\label{sec:eval-dedup}

%We map the traces with two different layer groups: 
%small layers (layer size $\leq$ 50 MB)

\input{fig-eval-1node}

In this experiment,
we measure the layer restoring latency of \sysname's deduplication cluster
and its impact on \texttt{GET} layer latency.
We first measure a single P-server's layer restoring capability
and compare it with a \emph{No deduplication} scheme --
downloading layers from server's local file system without deduplication.
%denoted as .
Then we scale out to multiple P-servers
and compare it with a \emph{CHT based distributed object storage} system.
%
%
% (denoted as
%\sysname-dedup)
% and 
%overhead of layer deduplication on .
%To measure the  
%We compare the \texttt{GET} layer latency of D-servers
%with 
%by running \sysname with \text{only deduplication} on a single D-server.
%We launch a client on another server and \texttt{pulls} 3 layers in parallel.
%We configure \sysname as
%registry without deduplication,
%deduplication registry,
%deduplication registry with LRU cache~\cite{xxx},
%and
%deduplication registry with a preconstruct cache.
%We set the cache size as 20\% of ingress data.

\paragraph{Restoring latency}
We launch a single node registry on a server.
and a client on another server.
We compare 4 backends:
(1) Local file system with \emph{no deduplication},
(2) Local file system with \sysname \emph{Deduplication} but without \emph{cache},
(3) Local file system with \sysname \emph{Deduplication} and a \emph{ARC cache}, and
(4) Local file system with \sysname \emph{Deduplication} and a \sysname \emph{preconstruct cache}
as shown in Figure~\ref{fig:eval-1nodegetlayerlatency}.
%
The client matches 2500 requests from \texttt{Dal} to layers with different sizes
and replays them to the D-server.
%We compare four backends: 

As shown in Figure~\ref{fig:eval-1nodegetlayerlatency},
layer restoring increses the average \texttt{GET} layer latency by 189\% for layers with size of 1 MB
 (\emph{see line Deduplication without cache})
compared with \emph{No deduplication}.
Moreover,
layer restoring latency increases as layer size.
It takes 0.22 s to restoring and download layers with size of 9 MB.

\paragraph{Preconstruct cache VS. ARC cache}
%\paragraph{Cache}
As shown in Figure~\ref{fig:eval-1nodegetlayerlatency}, 
%the average \texttt{GET} layer request latency increase with the layer size.
adding a cache can largely reduce layer restoring latency.
By using a ARC cache, the layer restoring overhead decrease by
40\% for layers with size of 1 MB compared with \emph{Deduplication without cache}.
However,
as layer size increase, 
the improvements of \texttt{GET} layer request latency by using
ARC cache drops.
For layer size equals to 10 MB,
%
ARC cache only reduces layer restoring overhead by 16\%.
%ARC cache can only improve 10\% of 
%Restoring a layer puts 150\% overhead on \texttt{GET} layer latency.
%By adding a ARC cache,
%he average \texttt{GET} layer latency decreases by 39\%.
With \sysname preconstruct cache,
the average \texttt{GET} layer latency descreases by 24\% over an ARC cache
for layers with size of 1 MB.
%Overall,
Compared to \emph{No deduplication}, 
\sysname with preconstruct cache only adds 19\% of overhead for layers with size of 9 MB. 

\paragraph{Cache hit ratio}
Figure~\ref{fig:eval-cachehitratios}
shows the cache hit ratio for ARC cache and Preconstruct cache.
Note that the cache size is set to 20\% of ingress data.
The cache hit ratio for ARC is stable at 0.77 among different layer sizes.
%However, 
Preconstruct cache achieves a hit ratio of 0.95 for 1 MB.
With the layer size increases to 9 MB,
preconstruct cache hit ratio decreases to 79\%. 
This is because layer restoring latency increases with layer size as 
shown in Figure~\ref{fig:eval-1nodegetlayerlatency}
and more layers cannot be preconstructed \emph{on time} as layer size becomes bigger. 
As shown in Figure~\ref{fig:eval-cachehitratios},
the number of waiting \texttt{GET} layer requests increases as layer size (see preconstruct cache waiting ratio).
Since the layer construction already starts before these requests arrives,
the restoring overhead can also be largely reduced.
 The user-bahavior based request prediction accuracy is calculated by sum of preconstruct cache hit ratio and waiting ratio.
 As shown in Figure~\ref{fig:eval-cachehitratios},
 the prediction accuracy is 0.95.
 
\paragraph{Impact of duration between a \texttt{GET} manifest request and its subsequent \texttt{GET} layer requests}
Next, 
we vary 
the duration between a \texttt{GET} manifest request and its subsequent \texttt{GET} layer requests.
The layer size used here is 10 MB.

Figure~\ref{xxx} shows the cache hit ratio of the ARC cache and the preconstruct cache.

\paragraph{Cluster scale impact}
We increase the number of nodes in \sysname deduplication cluster and test the layer restoring performance.

As shown in Figure~\ref{xxx},



Figure~\ref{xxx} shows the breakdown performance of layer restoring latency.


\paragraph{Client concurrency impact}





\subsection{Performance of primary cluster }
%\paragraph{}
In this experiment,
we test five modes of \sysname:
\textbf{B-mode 0},
\textbf{B-mode 1},
\textbf{B-mode 2},
\textbf{B-mode 3}, and
\textbf{S-mode}.
Note that B-mode 3 does not deduplicate layer replicas.
While B-mode 0 \emph{deduplicates} all layer replicas. 
%\sysname is evaluated by configuring different deduplication modes.
Therefore, B-mode 3 doesn't have P-servers while
B-mode 0 doesn't have P-servers.
For the remaining modes,
the number of P-servers and D-servers are set to 14 and 7 respectively.
%In addition, 
%we speedup trace replaying by using different speedup factors
%so that each trace can be finished within 30 minutes.
Before we replay each workload, we first warmup P-servers and D-servers with a certain amount of layers as shown in Table~\ref{tab:eval-overall}, which roughly takes 10 minutes.
Then, we use 32 clients spread between four client servers distributed on client cluster to replay traces to P-servers and D-servers.
We also evaluate the \emph{original registry} with 3-way replication on 21 servers. 

Figure~\ref{label} shows the average response time for \texttt{GET} layer, \texttt{PUT} layer, \texttt{GET} manifest, and \texttt{PUT} manifest requests for \sysname with different deduplication modes and original registry across different workloads.
Note that for \emph{B-mode 0}, the \texttt{GET} layer latency is the \emph{layer restoring latency}.

Overall, \texttt{PUT} requests have much higher average response time than \texttt{GET} requests.
For example, the average response time of 
\texttt{PUT} layer requests is almost twice higher than that of \texttt{GET} layer requests.
Note that registry servers use SSDs as secondary storage devices.
The difference in response time for \texttt{PUT} and \texttt{GET} requests are mainly caused by the different writes and reads throughput of SSDs. 

Comparing the \emph{original} with \emph{B-mode 3},
we see that \emph{B-mode 3} improves the overall performance by using a \emph{superfetch cache}.
\emph{B-mode 3}.
\emph{B-mode 3} reduces 5\% of average \texttt{GET} layer latency
and 32\% of average \texttt{PUT} layer latency compared with \emph{original} for workload \texttt{Dal}.
Next we compare \emph{B-mode 0} with \emph{original}.
Although \emph{B-mode 0} saves half of the storage space
by \emph{deduplicates} all layer replicas,
it slightly reduce \texttt{GET} layer latency by 6\%.
This is because of the high preconstruct cache hit ratio (detailed in Figure~\cite{xxx}), 
which reduces the layer restoring latency.

For the remaining basic modes: \emph{B-mode 2} and \emph{B-mode 1},
we see that \emph{B-mode 1} has a similar average response time to \emph{original}
although the primary cluster of \emph{B-mode 1} only has 14 nodes while 
\emph{original} contains 21 nodes.
This is because only a one-third of the %less data (1/3) 
data is \texttt{pushed} onto the primary cluster in \emph{B-mode 1}.
\texttt{Push} requests also affect the performance of \texttt{pull} requests
in terms of more network traffic and more writes to SSDs.
While \emph{B-mode 2} degrades \emph{GET} layer performance by 11\%.
This is because in \emph{B-mode 2},
more data (2/3) is \texttt{pushed} onto the 14-node primary cluster compared to \emph{B-mode 1},
which causes a lot of overhead.
\emph{S-mode} also has a similar \texttt{GET} layer performance with both \emph{original} and \emph{B-mode 1}.
This is because in \emph{S-mode}, only a few of the hot layers have 3 replicas while the remaining cold layers only have a single layer replica on the primary cluster. 

We find that
\emph{B-mode 1}, \emph{B-mode 2}, \emph{B-mode 0}, and \emph{S-mode}
 also improve the \texttt{PUT} layer performance by $\sim$ 30\% as \emph{B-mode 3}.
This is because
all 4 modes reduce the amount of \emph{pushed} layers to primary cluster results in
less network traffic and uses in-memory superfetch cache to reduce disk I/Os.
Similarly,
all modes improve \texttt{GET} and \texttt{PUT} manifest performance.
For example, \emph{B-mode 1} reduces \texttt{GET} and \texttt{PUT}
manifest latency by 17\% and 24\% respectively.

\paragraph{Cache hit ratio}

Figure~\ref{xxx},
shows superfetch cache hit ratio and preconstruct cache hit ratio.
Note that the superfetch cache is used by the primary cluster while the preconstruct cache is used by the deduplication cluster.
The superfetch cache hit ratio shown in Figure~\ref{xxx} is the average superfetch cache hit ratio across \emph{B-mode 3}, \emph{B-mode 2}, \emph{B-mode 1}, and \emph{S-mode} because they show the similar cache hit ratios.
Preconstruct cache hit ratio is measured in \emph{B-mode 0}.
Note that the big difference between a superfetch cache and a preconstruct cache is that the superfetch cache can prefetch layers \emph{on time} while the preconstruct cache cannot guarantee the preconstruction of layers \emph{on time} which makes a certain amount of \texttt{pull} layer requests \emph{wait} (detailed in~\ref{xxx}).
The cache size is set to 20\% of ingress data.

We see that the superfetch cache has a higher hit ratio than the preconstruct cache
because of the large amount of waiting layers on the preconstruct cache.
For example, for the preconstruct cache, 
the hit ratio is only 79\% excluding 20\% waiting layers requests for workload \texttt{Syd}.
while the hit ratio of the superfetch cache is 98\%.
The number of waiting layers vary across different workloads.
There are 4\% and 22\% of layer waits on the preconstruct cache for \texttt{Dal} and \texttt{Lon} respectively. 
Note that we speedup trace replaying
and the speedup factor for \texttt{Lon} is the biggest.
Trace replay speedup does not only decreases the idle time between \texttt{pull} image requests, it also reduces the duration between a \texttt{GET} manifest request and the subsequent \texttt{GET} layer requests.
Therefore, with the biggest speedup factor,
lots of layer preconstruction cannot be finished on time.
Moreover, for most of the workloads, the superfetch cache hit ratio is higher than 81\%. 
The \texttt{Dal} workload has the lowest hit ratio (0.77) because of the low prediction accuracy for \emph{users' repull behaviors} 
and the heaviness of the workload.

Figure~\ref{xxx} shows file cache hit ratio during layer restoring in \emph{B-mode 0}.
The file cache size is also set to 20\% of ingress data.
Overall, we see that file cache hit ratio is fair.
The hit ratio is lower than 60\% across all workloads, which means less files are shared among layers during a \emph{pull} image request.
It also means that the amount of shared/deduplicate files among layers is less when the accessed layer dataset is smaller.
Consequently, to get a higher deduplication ratio, deploying layer deduplication on a big layer dataset is a better choice, which aligns with~\cite{dedupanalysis}.


\paragraph{Metadata size} 
Figure~\ref{xxx} shows the average metadata size on each registry server measured by using the size of the used memory in Redis~\cite{redis} when in \emph{B-mode 0}.
Note that the replication level for the Redis cluster is also set to three.
As shown, for all traces, the total size of the metadata generated by \sysname such as \emph{recipes} for layers or slices, \emph{indexes} of layers or files, and RLmap or ULmap is less than 100 MB.
With the 16 GB memory on each registry server, such metadata size is negligible.

\paragraph{Client concurrency impact}
Figure~\ref{label} shows the average response time for different deduplication modes and for the original registry with 8 clients and 64 clients, respectively.

\paragraph{Primary cluster size  impact}
Figure~\ref{label} shows the average response time for different deduplication modes and for the original registry with a 7-node cluster and a 14-node cluster, respectively.
The number of concurrent clients are set to 32.
%\paragraph{Layer size impact}



%\subsubsection{Performance Vs. Space}

%\subsubsection{Performance }
%\section{Deduplication performance}
%\label{sec:Evaluation}
%
%%Our two-tier heterogeneous cache in the registry can improve the performance of the entire system
%%by hiding the long latency imposed by the backend dedup system.
%%Next, we present the preliminary evaluation of our user access history-based cache algorithm
%%and the space efficiency of file cache.
%
%\vspace{-6pt}
%\paragraph{Cache hit ratio.}
%
%We simulate our user-access-history-based cache and 
%replay the \texttt{dal} workload~\cite{dockerworkload} to measure the hit ratio. We set the cache size to be $20$\% of the data ingress for \texttt{dal}. We set $10$\% of the cache for buffering incoming \texttt{put} layer requests, and 
%the rest for caching prefetched layer slices from backend servers. 
%Note that in this evaluation, the cache only contains the layer buffer without the file cache.
%%as shown in Figure~\ref{fig:hitratio}.
%%Our algorithm exhibits an enhanced cache performance, with a high hit ratio
%%up to 0.96.
%Figure~\ref{fig:hitratio} shows the results. We observe a significant increase in the hit ratio, $74$\% to $95$\% as the duration threshold grows from $1$ to $10$ minutes. This is because prefetched layers are kept in the cache for more time.
%The hit ratio stablizes at $96$\% as the duration threshold increases from $15$ to $20$ minutes.
%%Therefore,
%%the highest hit ratio of our algorithm is around 0.96,
%%and there are 0.4 of layers that are miss because 
%%some users \emph{re-pull} the layers after they pull the same layers.
%The layers responsible for the $4$\% miss rate are the ones being
%\emph{re-pulled} by the same user.
%%We also see that 74\% of users can finish their \texttt{pull} layer request
%%within a minute and 
%%around 89\% of users can finished their \texttt{pull} layer request with less than 5 min. 
%%We also see that the response time to \texttt{pull} layer requests is within $1$ minute for 74\% of users, and it is less than $5$ minutes for 89\% of users.
%%Since we buffer layers upon a \texttt{push} layer request and prefetch layer {\em slices} from the backend servers, 
%%we compare the hits on prefetched layer {\em slices} and the hits on buffered layers.
%We see that, across different duration thresholds, 
%the hits upon buffering newly put requests (denoted as buffering hit ratio) is very low,
%confirming that it takes a long time for a recently \emph{pushed} layer to be pulled.
%We also observe a $22$\% average cache utilization. 
%That is because our algorithm is based on users demand 
%so it adapts to workload changes.
%%This trend is perfectly fit in our two-tier heterogeneous cache
%%and the evaluation results can guide us to carefully choose 
%%layer buffer size and file cache size.
%%
%%In terms of file count, it increases from \textbf{3.6$\times$} to \textbf{31.5$\times$} while
%%in terms of capacity, it increases from \textbf{1.9$\times$} to
%%\textbf{6.9$\times$} as the layer dataset grows from 1000 to 1.7 million layers.
%%%
%%This confirms the high potential for file-level deduplication in large-scale
%%Docker registry deployments.
%
%\vspace{-6pt}
%\paragraph{Space efficiency.}
%%As shown in Figure~\ref{xxx},
%%we compare the cache hit ratio of LRU, Prefetch~\cite{xxxx}, and our 
%%user-based cache replacement algorithm by replaying three IBM container registry workloads~\cite{dockerworkload}.
%%Our user-based cache exhibits an enhanced cache performance, with hit ratio improvements ranging from 
%%0.2 to 0.3 for all the three workloads compared to LRU.
%We analyze the space efficiency of the file cache compared to a cache that naively stores
%compressed layers.
%%for an increasing number of files stored in the file cache 
%%(see Figure~\ref{fig:dedup-ratio-growth}).
%%
%%Figure~\ref{fig:dedup-ratio-growth} shows the deduplication ratio growth over the layer dataset size.
%%
%In Figure~\ref{fig:cacheefficiency}, the x-axis values correspond to the sizes of $4$ random samples drawn from the whole dataset and the size of the dataset in terms of capacity and layer count.
%For a traditional cache, the compressed layer tarballs will be kept as is.
%While \sysname will store \emph{deduped} layers. 
%%unique files in the file cache.
%The y-axis shows how many more \emph{deduped} layers can fit in our file cache compared to naively storing compressed layer tarballs.
%For the first two samples of the dataset, with size less than $20$~GB, 
%there is no benefit to \emph{dedup} layers 
%because the deduplication ratio is very low.
%However, when the dataset size is $3$~TB, we can store $56\%$ more \emph{deduped} layers' unique files in file cache.
%The number of extra \emph{deduped} layers that can fit in the file cache increases almost linearly with the size of the layer dataset.
%%the bigger the dataset, the more deduped layers that can fit in the file cache.
%%The number of extra layers increases almost linearly with the layer dataset size.
%%In this case, there is a high potential for file cache when the cache size is big.
%This verifies the benefit of the file cache when the cache size is large, which should be carefully selected to realize significant space savings.


%In this experiment, we show how many more layers can be stored in the file cache 
%after decompression and file-level deduplication.
%Figure~\ref{xxxx} shows the growth of deduplication ratio with different dataset sizes.


%We evaluated \sysname's performance improvement over traditional 

%%While \sysname\ can effectively eliminate redundant files in the
%%Docker registry, it introduces overhead which can reduce the
%%registry's performance.
%%
%%The overheads can be classified in two categories: 1)~\emph{background
%%overhead} caused by the computation and I/O that is performed during layer
%%deduplication; and 2)~\emph{foreground overhead} from extra processing on the
%%critical path of a pull request.
%%\input{fig_dedup_res}
%
%
%%\paragraph{Hit ratios}
%%
%%\paragraph{Hit ratios with prefetching}
%%
%%%\subsection{} % what are the cost for a naive file-level deduplication
%%
%%\paragraph{Restoring performance breakdown}
%%
%%\paragraph{Simulation}
%%
%%To analyze the impact of file-level deduplication on the registry performance,
%%we conduct a preliminary simulation-based study of \sysname.
%%
%%Based on the simulation results, we estimated the overhead of \sysname\ on
%%\texttt{push} and \texttt{pull} layer request latencies.
%%
%%We then provide different suggestions on how the Docker registry can mitigate
%%the deduplication overhead.
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%
%Our simulation
%approximates several of \sysname's steps as described in Section~\ref{sec:design}.
%%
%First, a layer from our dataset is copied to a RAM disk. 
%%
%%
%%Note that there is no foreground pull or push requests since the simulation is \emph{off-line}.
%%
%The layer is then decompressed, unpacked, and the fingerprints of all files
%are computed using the MD5 hash function~\cite{MD5}.
%%
%The simulation searches the fingerprint index for duplicates,
%and, if the file has not been stored previously, it records the
%file's fingerprint in the index.
%%
%%To map a layer to its containing files, we create the layer recipe and add it
%%to a \emph{layer-to-file table}.
%%
%%The simulator then creates a file recipe.
%%
%%For each file in a layer, a layer digest
%%to its containing file content digest mapping record is also created 
%%
%%The \emph{layer-to-file table} also
%%records the file path within each layer associated with each file.
%%
%At this point our simulation does not include
%the latency of storing unique files.
%%
%To simulate the layer reconstruction during a \texttt{pull} request,
%we archive and compress the corresponding files.
%%
%%Only unique files are maintained in RAM
%%disk while the redundant copies are removed.
%%
%
%The simulator is implemented in 600 lines of Python code
%and our setup is a one-node Docker registry on a machine with 32~cores and 64\,GB of RAM.
%%
%To speed up the experiments and fit the required data in RAM
%we use 50\% of all layers and exclude the ones larger than 50\,MB.
%%
%We process 60 layers in parallel using 60 threads.
%%
%The entire simulation took 3.5 days to finish.
%%
%%The overall runtime is about 3.5 days.
%
%Figure~\ref{fig:dedup-res} shows the CDF for each sub-operation of
%\sysname.
%%
%Unpacking, Decompression, Digest Calculation, and Searching 
%are part of
%the deduplication process and together make up the Dedup time.
%%
%%\VT{@Nannan, in Figure ~\ref{fig:dedup-res} can you reorder the lines in the
%%legend so that the Searching goes after Digest calculation?}\NZ{addressed}
%%
%Searching, Archiving, and Compression
%simulate the processing for a \texttt{pull}
%request and form the Pulling time.
%%
%
%%\LR{What was the overall runtime for processing 0.9 million layers?}\NZ{addressed}
%%
%%\alicomment{How are we saving the location
%%of each file in the layer? It is not clear from the following sentences.}
%%\NZ{addressed}
%%
%%To improve searching performance, the
%%mapping table is stored in Hive database~\cite{xxx}. 
%%
%%\lrcomment{Why are we using Hive for this? It seems overkill to me, especially
%%for such small data. Even at scale, a KeyValue store would probably provide
%%better performance than clunky MapReduce-based DB.}
%%
%
%\paragraph{Push}
%
%\sysname\ does not directly impact the latency of \texttt{push} requests because
%deduplication is performed asynchronously.
%%ie the registry reliably stores a
%%copy of the layer as-is and then sends a response to the client.
%%
%The appropriate performance metric for \texttt{push} is the time it takes to deduplicate
%a single layer.
%%
%%Next, we look at the effects on \texttt{push} and \texttt{pull} latencies in
%%more detail.
%%
%%However, if there are intensive push requests while the registry is performing
%%deduplication, \sysname\ can still impact push latencies because it incurs
%%CPU, memory, and I/O overhead. %(similar to pull requests).
%%
%Looking at the breakdown of the deduplication time in
%Figure~\ref{fig:dedup-res}, we make several observations.
%
%First, the searching time is the smallest among all operations with 90\% of the
%searches completing in less than 4\,ms and a median of 3.9\,ms.
%%
%%The mapping table maintains 0.98 million layer-to-file digest mapping records. 
%%
%%\LR{Remove the following sentence? 1.7 million records is actually quite small
%%so even a single-node DB with one index is enough.}\NZ{addressed} Consider
%%that more than 1.7 million layers are stored in Docker hub and the number is
%%still increasing, it's better to choose a fast distributed database to provide
%%high searching performance and scalability.
%%
%Second, the calculation of digests spans a wide range from 5\,$\mu$s to almost
%125\,s.
%%
%%This is because the time mainly depends on the layer size, \ie the fewer and
%%smaller files a layer contains, the faster it is to compute all digests for
%%the layer.
%%
%%Typically, smaller layers contain a smaller number of smaller files, which
%%takes much less time to calculate their digests.
%%
%%While if the layer is bigger, the digest calculation overhead will be higher. 
%%
%90\% of digest calculation times are less than 27\,s while 50\% are
%less than 0.05\,s.
%%
%The diversity in the timing is caused by a high variety of layer sizes both in
%terms of storage space and file counts.
%%
%%Thus, we suggest that multiple-threading is needed to calculate the files'
%%digests simultaneously; 
%%
%%Fast CPUs as well as more powerful computing nodes are required to speed up
%%digest calculation.
%%
%Third, the run time for decompression and unpacking follows an identical
%distribution for around 60\% of the layers and is less than 150\,ms.
%%
%%Around 60\% of decompression and unpacking time are less than 0.15\,s. 
%%
%However, after that, the times diverge and decompression times increase faster
%compared to unpacking times.
%%
%%\VT{do we have some theory why?}
%%\NZ{decompressing the layers with bigger uncompressed size takes longer time.}
%%
%90\% of decompressions take less than 950\,ms while 90\% of packing time is less
%than 350ms.
%
%%Overall, we see that file digest calculation contributes a lot to the
%%overall deduplication latency especially when the layer size is big.  Moreover,
%%we see that the deduplication latency increases as the layer size grows.
%%
%Overall, we see that 90\% of file-level deduplication time is less than 35\,s
%per layer, while the average processing time for a single layer is 13.5\,s.
%%
%This means that our single-node deployment can process about 4.4\,layers/s on average
%(using 60 threads).
%%
%In the future we will work on further improving \sysname's deduplication throughput.
%%
%%In a large-scale registry deployment, this throughput can be improved
%%as more node are available to perform deduplication.
%%
%
%\paragraph{Pull} 
%
%From Figure~\ref{fig:dedup-res}
%we can see that 55\% of the layers have close compression and archiving
%times ranging from from 40\,ms to 150\,ms and both operations contribute equally
%to pulling latency.
%%
%%60\% of compression and archiving time are less than 0.15 s.
%%
%%While compression has the highest run time 80\% of compression time is less than 2.82~s. 
%%
%%\LR{Again, better to show the 90th percentile.}
%%\NZ{90\% of the compression time is less than 8\,s.}
%After that, the times diverge and compression times increase faster with an
%90\textsuperscript{th} percentile of 8\,s.
%%
%This is because compression times increase for larger layers and follow the distribution
%of layer sizes (see Figure~\ref{fig:layer-size-cdf}).
%%
%%80\textsuperscript{th} percentile of 2.82\,s.
%%
%Compression time makes up the major portion of the pull latency and is a
%bottleneck.
%%
%Overall, the average pull time is 2.3\,s.
%
%%
%%We see that archiving time and compression contributes equally to pulling
%%latency when their run time are lower than 0.15 s while compression time almost
%%equals to pulling latency when the compression time is greater than 0.15 s. 
