\subsection{Operations}
\label{sec:design_operations}

%\paragraph{Workflow}
%
The Docker registry API is almost the same as the original registry.
The user's interaction with the Docker client is unchanged. The user simply pushes and pulls images to and from the registry. In this subsection we explain Docker operations integration with \sysname.

%\sysname~ is composed of five main components(see Figure~\ref{onshareddir}):
%a modified Docker client,
%layer buffer,
%file cache,
%registries,
%backend storage system that implements deduplication.
%The modified Docker client sends put or pull layer/manifest requests.
%After it sends the pull layer requests.
%it receives multiple partial layers and decompresses them together as a whole uncompressed layer, then verifies the
%integrity of the uncompress layer by calculating the digest of the uncompress layer.
%Layer buffer is used to buffer all the put layer requests and 
%cache prefetched layers for later use.

\textbf{Push.}
After receiving a \texttt{push} layer request from the client, 
\sysname~first buffers the layer in the layer buffer for later accesses. 
The layer buffer can be implemented on a distributed in-memory store,~\eg Plusma.
Meantime, \sysname~will also submit this layer to the backend dedup storage system.\NZ{dedup}
Our layer buffer and file cache use write through policies. 
Since there is no modification to the layer or files, there is no data consistency issue between the cache and the backend dedup storage system.
A cold layer eviction might be triggered on layers stored in the layer buffer.
The cold layer will be evicted to the file cache, but first it will be deduped and then stored in file cache.
The deduplication process includes the following steps that are applied on every victim layer evicted from the layer buffer to the file cache:

\begin{compactenumerate}
	\item decompress and unpack the layer's tarball into individual files;
	\item compute a \emph{fingerprint} for every file in the layer;
	\item check every file's fingerprint against the \emph{fingerprint index} to
	identify identical files already present in the file cache;
	\item only store the unique files in the file cache and update the \NZ{unique file}
	\emph{file index} with unique files' fingerprints and file location along with its host address;
	\item create and store a \emph{layer recipe} that includes the file path,
	metadata, and fingerprint of every file in the layer;
	\item remove the layer's tarball from the layer buffer.
\end{compactenumerate}

Layer recipes are identified by layer digests and files are identified by their fingerprints.
These identifiers are used to address corresponding objects in the underlying flash storage. 
\emph{fingerprint index} and \emph{layer recipes} \HA{why are they in italic again?} are stored on Redis~\cite{redis}.

File cache is a flash-based distributed cache that can be implemented on distributed log structured store, \eg CORFU.
The unique files are flash-friendly because there is no modification to these files.
%, meaning that there is no small write.?
%
%Usually, the underlying distributed flash-based stores such as CORFU transparently spread data among 
%different servers and upper level applications are unaware of data host address. 
%We modified CORFU's read and write interfaces so that the file locations and 
%its host addresses are exposed to our ~\sysname.
%
%Similar to layer buffer, storing unique files in file cache might also trigger the eviction of cold layer's files.
The eviction of a cold layer from the layer buffer might also trigger the eviction of its files.
And since the backend dedup storage system already stores a backup of the layers, 
we can simply discard the victim files from the file cache.
The cache replacement algorithm is presented in section~\cref{sec:design_cache_algori}. 
%\sysname\ handles push requests asynchronously.
%\sysname 
%does not immediately unpack the layer.
%Instead, it reliably stores the layer's compressed tarball in a persistent
%\emph{staging area}.
%A separate \emph{off-line} deduplication process iterates over the layers in
%the staging area and performs the following steps for every layer:
%%
%\begin{compactenumerate}
%	\item decompress and unpack the layer's tarball into individual files;
%	\item compute a \emph{fingerprint} for every file in the layer;
%	\item check all file fingerprints against the \emph{file index} to
%	identify if identical files are already stored in \sysname;
%	\item store non-deduplicated files in \sysname's storage;
%	\item create and store a \emph{layer recipe} that includes the path,
%	metadata, and fingerprint of every file in the layer;
%	\item remove the layer's tarball from the staging area.
%\end{compactenumerate}

%The advantage of off-line deduplication is that it keeps push
%latencies perceived by the Docker clients low.
%
%The background deduplication process can be scheduled during the periods of low
%load on the registry.
%
%Layer recipes are identified by layer digests (see Section~\ref{sec:background})
%and files are identified by their fingerprints.
%%
%These identifiers are used to address corresponding objects in the
%underlying storage.
%
%For example, if a file system is used as a backend storage, \sysname\ creates a
%single file for every layer recipe (named by the digest) and a single file for
%every in-layer file (named by the fingerprint).

\paragraph{Pull}
A \texttt{pull} layer request that finds its desired layer in the layer buffer is a layer buffer hit. 
Otherwise, if it finds its containing files in file cache, that is a file cache hit and
 \sysname~has to \emph{reconstruct} the layer from the file cache based on the layer recipe.
%
%A pull request cannot be postponed to an off-line process as the
%pulling client is actively waiting for the layer.
%
\sysname\ performs the following steps \emph{inline} for restoring a layer from the file cache:

\begin{compactenumerate}
	%\item check if the requested layer is still in the staging area and if so,
	%service it directly from there;
	\item find the layer recipe by the layer digest and get the layers' containing files' \emph{fingerprints};
	\item lookup the \emph{fingerprints} in \emph{fingerprint index} to get a destination server list.
	\item forward the \texttt{pull} layer slice request and layer recipe to each server in the server list.
\end{compactenumerate}

Once the \texttt{pull} layer slice request is received, each destination server will initiate a layer slice restoring process 
which performs the following steps: 

\begin{compactenumerate}
	\item Prepares a directory structure for the layer based on the layer recipe;
	\item Copy the locally available files into the directory tree, 
	\item Compresses the layer's directory tree into a temporary tarball;
	\item Send the layer tarball back to the client and then discard the layer tarball.
\end{compactenumerate}

If A \texttt{pull} layer request is miss on both the layer buffer and the file cache, 
the request will be forwarded to the backend dedup storage system.
The layer restoring process on the backend storage system is similar to restoring from the file cache.
Many modern storage systems with the deduplication feature can be used as our backend storage system, including GFS~\cite{gfs}, HDFS~\cite{hdfs}, S3~\cite{s3}, and Swift~\cite{swift}.
We can modify the above systems so that they can recognize the compressed layer file type and decompress them before performing deduplication.
Moreover, the systems can restore layer slices in parallel. 
%Note that there is no consistency issue between either layer buffer, file cache or backend storage because layers won't be changed in the future.

\NZ{put in background or intro, conclusion, discussion?}
At a high-level, registries act like a distributed cache siting closer to clients to provide better performance in terms of response time. 
The layer buffer holds hot layers that belong to active users.
%To improve the capacity limitation of the main memory cache, we utilize flash memory to store unique files since it offers fast random read access.
The utilization of flash memory to store unique files not only mitigates the capacity limitation of the main memory cache, it also offers fast random read accesses.
% a on a distributed flash-based store.  \\
