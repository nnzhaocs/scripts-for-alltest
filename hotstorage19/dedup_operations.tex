\subsection{Operations}
\label{sec:design_operations}

%\paragraph{Workflow}
%
The registry API is almost the same as original registry.
The user simply pushes and pulles image to registry. 

%\sysname~ is composed of five main components(see Figure~\ref{onshareddir}):
%a modified Docker client,
%layer buffer,
%file cache,
%registries,
%backend storage system that implements deduplication.
%The modified Docker client sends put or pull layer/manifest requests.
%After it sends the pull layer requests.
%it receives multiple partial layers and decompresses them together as a whole uncompressed layer, then verifies the
%integrity of the uncompress layer by calculating the digest of the uncompress layer.
%Layer buffer is used to buffer all the put layer requests and 
%cache prefetched layers for later use.

\textbf{Push.}
After receiving a layer from the client, 
\sysname~first buffers the layer in layer buffer for later accesses. 
Layer buffer can be implemented on a distributed in-memory store,~\eg Plusma.
Meantime, \sysname will also submitted this layer to the backend dedup storage system.\NZ{dedup}
Our layer buffer and file cache use write through policies. 
Since there is no modification to the layer and files, so there is no data consistency issue between
cache and backend dedup storage system.
Storing a layer in layer buffer might trigger cold layer eviction.
The cold layer will first be evicted to file cache, deduped, then stored in file cache.
A deduplication process performs the following steps for every victim layer that is evicted from layer buffer to file cache.

\begin{compactenumerate}
	\item decompress and unpack the layer's tarball into individual files;
	\item compute a \emph{fingerprint} for every file in the layer;
	\item check all file fingerprints against the \emph{fingerprint index} to
	identify if identical files are already stored in the file cache;
	\item store only unique files in file cache and update the \NZ{unique file}
	\emph{file index} with file \emph{fingerprint} and file location along with its host address;
	\item create and store a \emph{layer recipe} that includes the file path,
	metadata, fingerprint of every file in the layer;
	\item remove the layer's tarball from the layer buffer.
\end{compactenumerate}

Layer recipes are identified by layer digests and files are identified by their fingerprints.
These identifiers are used to address corresponding objects in the underlying flash storage.
\emph{fingerprint index} and \emph{layer recipe} are stored on Redis~\cite{redis}.

File cache is a flash-based distributed cache that can be implemented on distributed log structured store, \eg CORFU.
The unique files are flash-friendly workload because there is no modification to these files, meaning that there is no small write.
%Usually, the underlying distributed flash-based stores such as CORFU transparently spread data among 
%different servers and upper level applications are unaware of data host address. 
%We modified CORFU's read and write interfaces so that the file locations and 
%its host addresses are exposed to our ~\sysname.
Similar to layer buffer, storing unique files in file cache might also trigger the eviction of cold layer's files .
Since the backend dedup storage system already stores the backup of the layers, 
we can simply discard the victim files from file cache.
The cache replacement is present in~\cref{sec:design_cache_algori}. 
%\sysname\ handles push requests asynchronously.
%\sysname 
%does not immediately unpack the layer.
%Instead, it reliably stores the layer's compressed tarball in a persistent
%\emph{staging area}.
%A separate \emph{off-line} deduplication process iterates over the layers in
%the staging area and performs the following steps for every layer:
%%
%\begin{compactenumerate}
%	\item decompress and unpack the layer's tarball into individual files;
%	\item compute a \emph{fingerprint} for every file in the layer;
%	\item check all file fingerprints against the \emph{file index} to
%	identify if identical files are already stored in \sysname;
%	\item store non-deduplicated files in \sysname's storage;
%	\item create and store a \emph{layer recipe} that includes the path,
%	metadata, and fingerprint of every file in the layer;
%	\item remove the layer's tarball from the staging area.
%\end{compactenumerate}

%The advantage of off-line deduplication is that it keeps push
%latencies perceived by the Docker clients low.
%
%The background deduplication process can be scheduled during the periods of low
%load on the registry.
%
%Layer recipes are identified by layer digests (see Section~\ref{sec:background})
%and files are identified by their fingerprints.
%%
%These identifiers are used to address corresponding objects in the
%underlying storage.
%
%For example, if a file system is used as a backend storage, \sysname\ creates a
%single file for every layer recipe (named by the digest) and a single file for
%every in-layer file (named by the fingerprint).

\paragraph{Pull}
A PULL layer request that finds its desired layer in layer buffer is a hit in layer buffer. 
Otherwise, if it finds its containing files in file cache, that is a hit in file cache and
 \sysname~has to \emph{reconstruct} the layer from file cache based on the layer recipe.
%
%A pull request cannot be postponed to an off-line process as the
%pulling client is actively waiting for the layer.
%
\sysname\ performs the following steps \emph{inline} for restoring a layer from file cache:

\begin{compactenumerate}
	%\item check if the requested layer is still in the staging area and if so,
	%service it directly from there;
	\item find the layer recipe by the layer digest and get the layers' containing files' \emph{fingerprints};
	\item lookup the \emph{fingerprints} in \emph{fingerprint index} to get a destination server list.
	\item forward the PULL split layer request and layer recipe to each server in the server list.
\end{compactenumerate}

Once the PULL split layer request is received, each destination server will initiate a split layer restoring process 
which performs the following steps. \NZ{layer split or split layer} 

\begin{compactenumerate}
	\item Prepares a directory structure for the layer based on the layer recipe;
	\item Copy the locally available files for the layer into the directory tree, 
	\item Compresses the layer's directory tree into a temporary tarball;
	\item Send the layer tarball back to the client and then discard the layer tarball.
\end{compactenumerate}

If A PULL layer request is miss on both layer buffer and file cache, 
it will be forwarded to backend dedup storage system.
The layer restoring process on backend storage system is similar to file cache.
Many modern storage systems with deduplication feature can be used as our backend storage system, such as GFS~\cite{gfs}, HDFS~\cite{hdfs}, S3~\cite{s3}, and Swift~\cite{swift}.
We can modify the above systems so that they can recognize compressed layer file type and decompress them before performing deduplication.
Also the systems can restore split layers in parallel. 
%Note that there is no consistency issue between either layer buffer, file cache or backend storage because layers won't be changed in the future.

\NZ{put in background or intro, conclusion, discussion?}
At a high-level, registries act like a distributed cache which sits close to clients to provide fast performance. 
Layer buffer holds hot layers for active users.
To improve the capacity limitation of main memory cache, we utilize flash memory to store unique files since it offers fast random read access.
% a on a distributed flash-based store.  \\
