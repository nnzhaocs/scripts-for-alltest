\subsection{Operations}
\label{sec:design_operations}

%\paragraph{Workflow}
%
The proposed Docker registry API is very similar to the original API.
%Interactions with the Docker client is unchanged: 
A user simply pushes and pulls images to and from the registry. 
In the following, we explain Docker operations integration with \sysname.

%\sysname~ is composed of five main components(see Figure~\ref{onshareddir}):
%a modified Docker client,
%layer buffer,
%file cache,
%registries,
%backend storage system that implements deduplication.
%The modified Docker client sends put or pull layer/manifest requests.
%After it sends the pull layer requests.
%it receives multiple partial layers and decompresses them together as a whole uncompressed layer, then verifies the
%integrity of the uncompress layer by calculating the digest of the uncompress layer.
%Layer buffer is used to buffer all the put layer requests and 
%cache prefetched layers for later use.

\textbf{Push.}
After receiving a \texttt{push} layer request from the client, 
\sysname~first buffers the layer in the layer buffer for later accesses,
which is part of our layer prefetching (detailed in algorithm~\ref{alg:prefetch}). 
%Layer buffer also caches prefetched layers.
The layer buffer can be implemented on a distributed in-memory store,~\eg Plasma~\cite{plasma}.
At the same time, \sysname~will also submit the layer to the backend dedup storage system.
Our layer buffer and file cache use write through policies. 
Since there is no modification to the layer or unique files, 
there is no data consistency issue between the cache and the backend dedup storage system.
A cold layer eviction may be triggered on layers stored in the layer buffer.
The cold layer will be evicted to the file cache, but before that can happen, it will be \emph{deduped}.
The deduplication process includes the following steps 
that are applied on every victim layer evicted from the layer buffer to the file cache:

\begin{compactenumerate}
	\item decompress and unpack the layer into files;
	\item compute a \emph{fingerprint} for every file in the layer;
	\item check every file's fingerprint in the \emph{fingerprint index} to
	ensure an identical file is not already cached;
	\item only store the unique files in the file cache and update the 
	\emph{file index} with the unique files' fingerprints, location, and its host address;
	\item create and store a \emph{layer recipe} comprising the file path,
	metadata, and fingerprint of the layer's file; and
	\item remove the layer's tarball from the layer buffer.
\end{compactenumerate}

Layer recipes and files are identified by layer digests and fingerprints, respectively, and 
are used to address corresponding objects in the flash storage. 
Fingerprint index and layer recipes are stored on Redis~\cite{redis}.

File cache is a flash-based distributed cache that can be implemented on 
distributed log structured store, \eg CORFU~\cite{180277}.
The unique files are flash-friendly because there is no modification to these files.
%Both layer buffer and file cache cache prefetched layers.
%, meaning that there is no small write.?
%
%Usually, the underlying distributed flash-based stores such as CORFU transparently spread data among 
%different servers and upper level applications are unaware of data host address. 
%We modified CORFU's read and write interfaces so that the file locations and 
%its host addresses are exposed to our ~\sysname.
%
%Similar to layer buffer, storing unique files in file cache might also trigger the eviction of cold layer's files.
The eviction of a cold layer from the layer buffer to file cache
 may also trigger the eviction of unique files in file cache.
Since the backend dedup storage system already stores a backup of the layers, 
we can simply discard the victim files.
%  from the file cache. Algorithm~\ref{alg:cache} presents our 
% cache replacement algorithm.
%\sysname\ handles push requests asynchronously.
%\sysname 
%does not immediately unpack the layer.
%Instead, it reliably stores the layer's compressed tarball in a persistent
%\emph{staging area}.
%A separate \emph{off-line} deduplication process iterates over the layers in
%the staging area and performs the following steps for every layer:
%%
%\begin{compactenumerate}
%	\item decompress and unpack the layer's tarball into individual files;
%	\item compute a \emph{fingerprint} for every file in the layer;
%	\item check all file fingerprints against the \emph{file index} to
%	identify if identical files are already stored in \sysname;
%	\item store non-deduplicated files in \sysname's storage;
%	\item create and store a \emph{layer recipe} that includes the path,
%	metadata, and fingerprint of every file in the layer;
%	\item remove the layer's tarball from the staging area.
%\end{compactenumerate}

%The advantage of off-line deduplication is that it keeps push
%latencies perceived by the Docker clients low.
%
%The background deduplication process can be scheduled during the periods of low
%load on the registry.
%
%Layer recipes are identified by layer digests (see Section~\ref{sec:background})
%and files are identified by their fingerprints.
%%
%These identifiers are used to address corresponding objects in the
%underlying storage.
%
%For example, if a file system is used as a backend storage, \sysname\ creates a
%single file for every layer recipe (named by the digest) and a single file for
%every in-layer file (named by the fingerprint).

%\vspace{-4pt}
{\bf Pull.}
A \texttt{pull} layer request that can be serviced from the 
layer buffer is a layer buffer hit and does not need further action. 
In case of a miss in the layer buffer, the needed files may be found in the 
file cache. If this happens, 
 \sysname~\emph{reconstructs} the layer from the file cache according to the layer recipe.
%
%A pull request cannot be postponed to an off-line process as the
%pulling client is actively waiting for the layer.
%
The layer reconstruction from the file cache involves the following steps that are performed \emph{inline}:

\begin{compactenumerate}
	%\item check if the requested layer is still in the staging area and if so,
	%service it directly from there;
	\item find the layer recipe using the layer digest and retrieve the 
\emph{fingerprints} for the files associated with the layer;
	\item lookup the \emph{fingerprints} in \emph{fingerprint index} to get a destination server list; and
	\item forward the \texttt{pull slice} %layer 
	request and layer recipe to each server in the server list.
\end{compactenumerate}

Once the \texttt{pull slice} 
%layer 
request is received, each destination server will initiate a layer slice restoring process. This process involves  
the following steps: 

\begin{compactenumerate}
	\item prepare a directory structure for the layer, based on the layer recipe;
	\item copy the locally available files into the directory tree; 
	\item compresses the layer's directory tree into a temporary tarball; and
	\item send the layer tarball back to the client, and then discard the tarball.
\end{compactenumerate}

If a \texttt{pull} layer request results in a miss in both the layer buffer and the file cache, 
the request will be forwarded to the backend dedup storage system.
The layer restoring process on the backend storage system is similar to restoring from the file cache.
Many modern storage systems with the deduplication feature can be used as our backend storage system, including GFS~\cite{ghemawat2003google}, HDFS~\cite{hdfs}, S3~\cite{s3}, and Swift~\cite{swift}.
We can modify the above systems so that they can recognize the compressed layer file type and decompress them before performing deduplication.
Moreover, these systems need to be modified to restore layer slices in parallel. 
%Note that there is no consistency issue between either layer buffer, file cache or backend storage because layers won't be changed in the future.


