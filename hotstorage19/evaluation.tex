\section{ Evaluation}
\label{sec:Evaluation}

For the evaluation of \sysname, we are interested in two main aspects. First, the deduplication
performance of \sysname happening in the D-cluster~(\S\ref{sec:eval-dedup}).
Second, the effect of different deduplication modes on the performance of the primary cluster~(\S\ref{sec:eval-primary}).

%Our two-tier heterogeneous cache in the registry can improve the performance of the entire system
%by hiding the long latency imposed by the backend dedup system.
%Next, we present the preliminary evaluation of our user access history-based cache algorithm
%and the space efficiency of file cache.

%We conduct a performance evaluation of \sysname, and compare it with the modern registry setup~\cite{dockerworkload}.
%We start with describing the testbed and workloads then follow that with the evaluation results.
\subsection{Evaluation Setup}

Our testbed consists of 
%two clusters: 
%8-node client cluster and
%8-node registry cluster (Hulks), and
a 16-node cluster with
%Each client server is equipped with 32 cores, 64 GB RAM, 500 GB SSD, and 10,000Mb/s NIC. 
%Each client server is equipped with 8 cores, 64 GB RAM, 1 TB HDD, and 1000Mb/s NIC.
%During evaluation,
each node being equipped with 8 cores, 16\,GB RAM, a 500\,GB SSD, and a 10\,Gbps NIC. 
%We deploy Docker's registry container on each server and deploy a distributed Redis cluster on these 22 registry servers as our database.
%During our evaluation,
%we select different number of servers as our registry cluster
%and use the rest as client server.
%Note that each server only runs a single registry container
%while multiple clients can run on the same server.
%Besides,

%We instantiate multiple clients on each client server.
%backend registry storage cluster and frontend registry cache cluster. 
%Backend storage cluster includes 8 servers. 
%Each registry Hulk server is equipped with 32 cores, 64 GB RAM, 500 GB SSD, 1 TB HDD, and 10,000Mb/s NIC. 

%We implemented \sysname~cache on frontend cache cluster and installed \sysname~dedup on backend storage cluster. 
%We use extra 5 machines and each machine launches different number of clients to emulate client requests.
%Hulks and new hulks are used as backend.
%Thors are used as frontend.
%\subsection{}
\paragraph{Dataset}
%
%Recent work \cite{analysisdockergithub} conducts a comprehensive analysis of 47~TB Docker Hub images and reports that the popularity distribution of the images is heavily skewed.
%To evaluate how \sysname works for frequently accessed images, 
As our dataset, we download 74,000 popular images (i.e., images with a pull count greater than 100) from Docker Hub.
% 
The total size of this dataset is 12.5\,TB and contains 507,023 layers with 87.5\% of them being smaller than 50\,MB.
%
After decompressing the layers, the total size of the dataset is 27.7\,TB.
%
Applying file-level deduplication on the decompressed dataset
reduces the total size of the dataset to only 7.2\,TB, yielding 
a deduplication ratio of 1.74.
%
%\Subil{is the dedup ratio $7.2\div27.7$ or $7.2\div12.5$? because neither of them equals 0.5. How is the dedup ratio calculated?}.
%
%The size distribution of our popular layer dataset is the same with the size distribution of~\cite{analysisdockergithub}.
%87.5\% of layers are smaller than 50 MB.
%12.3\% of layers fall between 50 MB and 1 GB.
%0.2\% of layers are bigger than 1 GB.
%Due to our testbed cluster's scaling restriction, 
%we set a limit to the layer size %\emph{layer size limitation} 
%at 250 MB. 
%Layers that are bigger than 250 MB are split into sub-layers.
%Based on our dataset analysis~\cite{xxx} and workload analysis~\cite{xxx},
%more than 90\% of layers are smaller than 50MB. 
%Therefore, we divided our sample dataset into two groups:
%smaller layers (layer size $\leq$ 50 MB) and big layers ( 50MB $\leq$ layer size $\leq$ 1 GB).
  
\paragraph{Workload}
%
To evaluate how \sysname performs for production registry workloads, 
we use the IBM Cloud Registry traces~\cite{dockerworkload}.
%
The traces come from three private registry clusters, 
four production registry clusters and span approximately 80 days.
%
As detailed in~\S\ref{sec:impl}, we randomly match layers from the four anonymized production
traces (\dal, \fra, \lon, and \syd) to layers of our dataset to generate four real-world production workloads.
%
Before each experiment, we preload all unique layers to the P- and D-servers
and then replay the first 5,000 requests from each workload.
%We do not warm-up the caches.
%
%\Subil{there's nothing in the table indicating what number of layers we use for warmup.}
%\NZ{the unique layer count is the number.}
%\LR{Do you mean we warm up the caches?}
%\NZ{No we dont warmup caches.
%The warmup requests wont go to caches.
%We distinguish the request types as: \texttt{GET}, \texttt{PUT}, and \texttt{WARMUP}, 
%\texttt{PRECONSTRUCT}.}
%
Table~\ref{tab:eval-overall} details how the 5,000 requests for each workload are composed
with respect to layer and manifest requests.

%Note that we use a 3-way replication
%which triples the amount of data written during evaluation.
%We speedup trace replaying by using different speedup factors so that each trace finishes within 30 minutes.
%During all experiments,
%we set \emph{replication level} equal to 3.

\input{tab-eval-overall}
\input{eval-performance}
\input{eval-primary}
%\input{eval-restore-performance}
%\input{eval-dedup}
