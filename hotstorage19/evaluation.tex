\section{ Evaluation}
\label{sec:Evaluation}

%Our two-tier heterogeneous cache in the registry can improve the performance of the entire system
%by hiding the long latency imposed by the backend dedup system.
%Next, we present the preliminary evaluation of our user access history-based cache algorithm
%and the space efficiency of file cache.

%We conduct a performance evaluation of \sysname, and compare it with the modern registry setup~\cite{dockerworkload}.
%We start with describing the testbed and workloads then follow that with the evaluation results.
\subsection{Testbed, workloads and dataset}

Our testbed includes 
%two clusters: 
%8-node client cluster and
%8-node registry cluster (Hulks), and
a 15-node cluster.
%Each client server is equipped with 32 cores, 64 GB RAM, 500 GB SSD, and 10,000Mb/s NIC. 
%Each client server is equipped with 8 cores, 64 GB RAM, 1 TB HDD, and 1000Mb/s NIC.
%During evaluation,
Each node is equipped with 8 cores, 16 GB RAM, 500 GB SSD, and 10,000 Mb/s NIC. 
%We deploy Docker's registry container on each server and deploy a distributed Redis cluster on these 22 registry servers as our database.
%During our evaluation,
%we select different number of servers as our registry cluster
%and use the rest as client server.
%Note that each server only runs a single registry container
%while multiple clients can run on the same server.
%Besides,

%We instantiate multiple clients on each client server.
%backend registry storage cluster and frontend registry cache cluster. 
%Backend storage cluster includes 8 servers. 
%Each registry Hulk server is equipped with 32 cores, 64 GB RAM, 500 GB SSD, 1 TB HDD, and 10,000Mb/s NIC. 

%We implemented \sysname~cache on frontend cache cluster and installed \sysname~dedup on backend storage cluster. 
%We use extra 5 machines and each machine launches different number of clients to emulate client requests.
%Hulks and new hulks are used as backend.
%Thors are used as frontend.
%\subsection{}
Recent work \cite{analysisdockergithub} conducts a comprehensive analysis of 47~TB Docker Hub images and reports that the popularity distribution of the images is heavily skewed.
To evaluate how \sysname works for frequently accessed images, 
we download 74,000 popular images (i.e., images with a pull count greater than 100) from Docker Hub~\cite{docker-hub} as our evaluation dataset. 
The size of this dataset is 12.5~TB and contains 507,023 layers. 
After decompressing the layers, the total size of the dataset is 27.7~TB.
We apply file-level deduplication on the decompressed dataset, 
the total size of the resulting dataset of unique files is only 7.2~TB, 
making the deduplication ratio 0.5 \Subil{is the dedup ratio $7.2\div27.7$ or $7.2\div12.5$? because neither of them equals 0.5. How is the dedup ratio calculated?}.
The size distribution of our popular layer dataset is the same with the size distribution of~\cite{analysisdockergithub}.
87.5\% of layers are smaller than 50 MB.
%12.3\% of layers fall between 50 MB and 1 GB.
%0.2\% of layers are bigger than 1 GB.
%Due to our testbed cluster's scaling restriction, 
%we set a limit to the layer size %\emph{layer size limitation} 
%at 250 MB. 
%Layers that are bigger than 250 MB are split into sub-layers.
%Based on our dataset analysis~\cite{xxx} and workload analysis~\cite{xxx},
%more than 90\% of layers are smaller than 50MB. 
%Therefore, we divided our sample dataset into two groups:
%smaller layers (layer size $\leq$ 50 MB) and big layers ( 50MB $\leq$ layer size $\leq$ 1 GB).
  
To evaluate how \sysname performs for production registry workloads, 
we leverage IBM's cloud registry workload traces\cite{dockerworkload}. 
The traces span $\sim$80 days from three private registry clusters and 
four production registry clusters. 
We randomly match the four anonymized production traces (i.e., \texttt{Dal, Fra, Lon, Syd}) 
with our dataset to form four real-world production workloads (detailed in Section~\ref{sec:impl}). 
We replay the first 5000 requests from each workload.
Tables~\ref{tab:eval-overall} detail the 5000 requests for each workload.
Before we replay each workload, 
we first warmup P-servers and D-servers with a certain amount of layers 
as shown in Table~\ref{tab:eval-overall} \Subil{there's nothing in the table indicating what number of layers we use for warmup}.
%Note that we use a 3-way replication
%which triples the amount of data written during evaluation.
%We speedup trace replaying by using different speedup factors so that each trace finishes within 30 minutes.
%During all experiments,
%we set \emph{replication level} equal to 3.

\input{tab-eval-overall}
\input{eval-performance}
\input{eval-primary}
%\input{eval-restore-performance}
%\input{eval-dedup}