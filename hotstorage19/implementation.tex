\section{Implementation}
\label{sec:impl}

We implement \sysname in Go on top of the existing Docker registry. In our implementation,
we address a set of challenges regarding distributed, reliable handling of
metadata~(\S\ref{sec:handling-metadata}), dealing with non-regular files~(\S\ref{sec:deduplication-details}),
and generating a suitable workload~(\S\ref{sec:workload}).

\subsection{Handling metadata}
\label{sec:handling-metadata}

\sysname{}'s metadata database needs to be reliable and fault tolerant to always be
able to correctly restore deduplicated layers in case of a failure.
%
\sysname uses Redis~\cite{redis} to store its metadata, i.e.
the slice and layer recipes, the file and layer indeces, and ULmap and RLmap.
%
We enable \emph{append-only file (AOF)} to log all changes for durability purposes.
This allows us to rebuild the dataset and prevent data loss after a Redis restart.
%Thus when Redis server stops, we can restart Redis and re-play the AOF to rebuild the dataset to prevent data loss.
%
Moreover, we configure Redis to save RDB (Redis Database File) snapshots every few minutes for additional
reliability. To improve availability, we configure Redis to use 3-way replication.

%The cluster membership is managed by Apache Zookeeper.
%It is used to detect failures in P-servers and handle failover to D-servers, 
%which are used to restore and serve layers when P-servers are down.
%In this case, Redis will not become a performance bottleneck.

%\paragraph{Distributed deduplication}
%
To ensure that the metadata is in a consistent state, \sysname uses Redis as a distributed
lock to make sure that no file duplicates are stored in registry cluster.
%
For the file index and layer indeces and the slice and layer recipes,
each key can be set to hold its value only if
the key does not yet exist in Redis (i.e, using \texttt{SETNX}~\cite{SETNX}).
%
When a key already holds a value, a file duplicate or
layer duplicate is identified and is removed from the registry cluster.

Additionally, \sysname maintains a synchronization map to ensure that multiple layer
restoring processes don't attempt to restore the same layer simultaneously.
%
If a layer is currently being restored, subsequent
\texttt{GET} layer requests to this layer wait until the layer is restored.
%
Other layers, however, can be constructed in parallel.
%
In this case, redundant layer restoring is avoided to save I/O, network, and
computation resources.
 %Slice restoring is the same.
%  Another synchronization technique present is a lock acquirable on a layer recipe 
%  that forces incoming requests to a D-server to wait 
%  if it is requesting a layer which has not fully deduplicated. 
%  Deduplication must complete before the restoring request can be served.
 
 
\subsection{Deduplication details}
\label{sec:deduplication-details}

%\paragraph{Non-regular files}
%
When deduplicating layers, \sysname needs to be able to deal with
non-regular files such as symlinks, device files, or directories.
%
%During layer deduplication, only \emph{regular} file duplicates~\cite{xxx} are removed.
%\sysname does not deduplicate \emph{irregular} files, such as symlinks, hard links, or directories.
%
Therefore, \sysname only considers regular files for deduplication.
%
After removing regular file duplicates, any remaining unique files are saved in a local file store.
%
%The file store is implemented by modifying the Docker registry's file system driver~\cite{dockerfs},
%which is a local storage driver that uses a directory tree in the local file system.

/var/lib/registry/docker/registry/v2 is the default root directory in the local file system, where all
registry-related data is stored (denoted as rootdir).
%
On the deduplication cluster,
each uploaded layer is stored in directory rootdir/blobs/sha256/ld.2/ld/
as a file named \texttt{data} where ld is the layer's hex
digest and ld.2 are the first two digits of the hex digest.
%
When deduplication has finished, \texttt{data} is deleted.

During deduplication, all \emph{regular} unique files are distributed to their
corresponding D-servers and stored there locally under
rootdir/uniquefiles/sha256/fd.2/fd/, where fd is the file's hex
digest and fd.2 its first two digits.
%
Every \emph{non-regular} file will be stored on the D-server, which initially
received the layer for deduplication, under rootdir/blobs/sha256/ld.2/ld/tmp.
%
When a layer is reconstructed, the restoring master can check for non-regular files through
the layer digest under that location.
%so that given a file digest, we can build the file path to save metadata information.
%

%\paragraph{Parallel (de)compression}
%
\LR{A bit out of context, not sure where to put the following to integrate it better.}
%To speedup layer restoring,
To speed up (de)compression, we use the parallel gzip library (pgzip)~\cite{pgzip}
for layer and slice compression/decompression.
%
Unlike Go's single-threaded standard gzip compression library~\cite{gogzip},
pgzip splits the compression stream into blocks so that data can be compressed in
parallel. pgzip is compatible with the standard gzip library.

%\paragraph{Dealing with big layers}
%%
%\LR{Can we elaborate a bit more on this? What exactly are the scale constraints and why
%do they prevent us from dealing with large layers?}
%%
%Due to the scale constraint of our testbed cluster, we limit the layer size
%to 250 MB, to reduce their restoring overhead.
%%
%\DIM{Does that constraint prevent us from seeing any Sift disadvantages in the evaluation?}
%%
%If  a layer that exceeds the size limitation is received,
%\sysname will first divide the big layer into sub-layers and update its associated manifests.
%%
%\LR{This only holds for deduplicated layers, correct? So we first decompress them and then split
%them in sub-layers? How are we doing the split?}
%%
%After that, the sub-layers are deduplicated and restored as normal layers.
%%
%Once a client receives the sub-layers it decompresses them into a single (big) layer.
%%
%To do so, the big layer manifest is modified to reflect the relation
%between the big layer and its sub-layers.
%%
%\DIM{Is this part of the Sift design, would it still be there in a real cluster, can it be disabled/enabled without negative consequences?}

%\paragraph{Cache implementation}
%%
%\LR{We could remove the entire paragraph.}
%%
%We implement both the preconstruct and superfetch cache by using Diskv~\cite{diskv},
%a disk-backed key-value store.
%%
%\LR{If we remove file cache from Design, we have to remove it here also.}
%%
%The file cache for file store is implemented on BigCache~\cite{bigcache},
%which provides fast, concurrent cache access.
%%
%%However, both BigCache and Diskv do not provide any cache evication algorithm.
%%Specifically, BigCache evicts the oldest entries from cache without considering the entries' popularity.
%%
%To support the ARC evication policy, we implemented two
%ARC lists for BigCache and Diskv respectively by using a golang cache library~\cite{xxx}.
%%
%The library provides expirable cache with various cache algorithms, such as LFU, LRU and
%ARC~\cite{megiddo2003arc}.
%
%In our implementation, we use ARC with expiration as our cache algorithm
%for both BigCache and Diskv.
%%and combine ARC cache algorithm with BigCache as our file cache
%%and ARC cache algorithm with Diskv as our preconstruct and superfetch cache.
%%
%Note that the preconstruct cache is an on-disk cache while the superfetch cache is an in-memory
%cache.
%%
%We mount a \texttt{tmpfs} disk for the superfetch cache to act as an in-memory cache.

%\paragraph{Cluster Management}

\subsection{Workload generation}
\label{sec:workload}

%\paragraph{Client implementation}
%For simply implementation,

To test \sysname{}'s ability to deduplicate, we need a representative, production
registry workload.
%
We base our workload generator on the IBM cloud registry trace replayer~\cite{dockerworkload}.
%
%Client is comprised of a trace replayer and a proxy emulator.
%The trace replayer is implemented by modifying the IBM cloud registry trace replayer~\cite{dockerworkload}.
However, as the traces do not contain specific image data and the trace
replayer only generates random layers, we modify the replayer
to match requested layers in the IBM cloud registry trace with
real layers, downloaded from Docker Hub.
%~\cite{dedupanalysis}.
%
Specifically, we extract layer digests from layer requests recorded in the IBM trace
and then randomly match each digest to a layer in Docker Hub
%. 
Consequently, each layer request pulls or pushes a real layer.
%
For manifest only requests recorded in the IBM traces, we generate a random file to emulate a
manifest file.
%This is because Sift does not deduplicate manifest file.
%The trace replayer uses Docker registry client emulator Python-dxf~\cite{pydxf} to 
%\texttt{pull} or \texttt{push} layers or manifests to registry.
%In this case, each layer request 
%will \texttt{pull} or \texttt{push} a real layer.

In addition, our workload generator uses a proxy emulator to decide
%the destination among the P-servers for each request.
the P-server for each request.
%
The proxy emulator uses
consistent hashing ~\cite{kargercons} to distribute layers and manifests.
%
It maintains a ring of registry servers
and calculates a destination registry server for each \texttt{push} layer or manifest request
by hashing its digest.
%
For \texttt{pull} manifest requests, the proxy emulator maintains two consistent hashing rings,
one for the P-servers, and one for the D-servers.
%
By default, it first queries the P-servers but if the requested P-server is not available,
it pulls from the D-servers.

%We implemented our preconstruction cache and \filecachename file cache on Redis cluster
%deployed by using Docker swarm, denoted as cache service.
%
%We decoupled our \sysname~implementation into cache implementation and backend dedup storage implementation. 
%We first modify the Docker registry source code -- local file system driver part by implementing decompression/compression, file-level deduplication, and 
%unique file distribution modules to the local file system driver so that the driver decompresses the layers and removing the duplicate files once the registry receives a layer tarfile, and fetching and compress the files into layer slices once the registry receives a get layer request. 
%We also implemented a simple round-robin based distributed key-value object store by modifying the local file system driver. 
%Once the file-level deduplication is done, the driver will re-distributed the newly added unique files to different destination servers based on round-robin. 
%Besides, we uses Zookeeper as the cluster coordinator, and MongoDB to store dedup metadata.
%
%Our \sysname~cache is also implemented by modifying Docker registry source code -- cache part. 
%The original Docker registry only caches local layer digests to identify if a requested layer is stored locally or not. 
%We first deploy our cache cluster by using Redis cluster. 
%We add our \sysname~cache into Docker registry source code as a new cache module which makes prefetch decisions when a GET/PUT request is received and talks to Redis cache. The metadata tables used by cache is also stored in MongoDB. For cache demotion/eviction and monitoring cache space utilization/backend dedup storage system performance, we elect a master registry node to do that by using Zookeeper as the cluster coordinator. 
%
%Therefore, by modifying Docker registry source code, we can configure Docker registry to provide two different kinds of functionalities. It can be either used as a distributed backend layer dedup storage system with the same Docker registry APIs or used as a distributed user-oriented cache with the same Docker registry APIs.

%\paragraph{Gzip compression}
